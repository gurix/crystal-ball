\documentclass[english,12pt,doc]{apa}
\usepackage[a4paper, left=25mm, right=40mm]{geometry}

\usepackage{apacite}
\usepackage[german]{babel}
\usepackage{verbatim}
\usepackage{setspace}
\onehalfspacing
\setlength{\parskip}{1em} % 1ex plus 0.5ex minus 0.2ex}
\setlength{\parindent}{0pt}

\usepackage[utf8]{inputenc}

\usepackage[]{blindtext}
\begin{document}

\title{Automatische Verfahren zur Prädiktorauswahl in Regressionsmodellen}
\shorttitle{Prädiktorauswahl in Regressionsmodellen} 
\author{Literaturarbeit vorgelegt von \\ Markus Graf (markus.graf@uzh.ch)}
\date{\today}
\affiliation{am  Psychologisches Institut der Universität Zürich\\ Betreut durch Dr. Christina Werner}
\abstract{Eigener Abstract: - 

Themenvorgabe: In vielen psychologischen Bereichen geht es darum, Kriteriumsvariablen durch Prädiktorvariablen möglichst gut vorherzusagen. Wenn viele potentielle Prädiktorvariablen in Frage kommen und es keine theoretischen Gründe gibt, die nur ganz bestimmte Prädiktorvariablen nahelegen, werden in Anwendungssituationen oft automatische Verfahren der Auswahl von Prädiktorvariablen verwendet, um mit möglichst wenigen Prädiktoren eine möglichst gute Vorhersage des Kriteriums zu erreichen, beispielsweise die sog. ``Stepwise''-Methode in multiplen Regressionsmodellen. Die Literaturarbeit soll einen Überblick über verschiedene existierende Möglichkeiten zur Selektion von Prädiktorvariablen in Regressionsmodellen geben, und deren Eignung für psychlogische Anwendungen kritisch diskutieren.}
\maketitle
\setlength{\parindent}{0pt}
\newpage
\tableofcontents
\newpage
\section{Einführung}
Das Standardverfahren um den quantitative Zusammenhang zwischen einer abhängigen und einer unnabhängigen Variablen zu beschreiben stellt die Regressionsanalyse dar. Begründet wurde dieses Verfahren durch Carl Friedrich Gauss in seiner Schrift, in der er, mit Hilfe der Methode der kleinsten Quadrate, die Bewegung der Himmelskörper um die Sonne im Kegelschnitt beschrieb \cite{gauss1809theoria}. 

Im Unterschied zur einfachen linearen Regression, werden in einem multiplen Regressionsmodel mehrere unabhängige Variablen mit einbezogen. Es resultiert eine Regressionsgleichung welche zur Vorhersage einer Kriteriumsvariable aufgrund mehrerer Prädikatorvariablen genutzt wird  \cite[S. 448]{bortz2011}. 
Die Gretchenfrage stellt sich nun welche Prädikatoren nun ein Model am besten erklären. Zu beginn der Psychologischen Forschung mussten Modelle von Hand berechnet werden. Zwangsläufig wurden wenige Prädikatoren erhoben und einfache modelle Gerechnet. Friedman analysierte beispielsweise 1944 die Langlebigkeit von Turbinenschaufeln in Abhängikeit von Stress, Temperatur und einigen Legierungsparametern. Zwar wurde die Berechnung nicht mehr von Hand durchgeführt, doch benötigte eine Regressionsschätzung inklusive Berechnung der Teststatistiken rund 40 Stunden \cite[p.2]{armstrong2011illusions}. Jeder durchschnittliche Computer erledigt dies heutzutage Sekundenbruchteile. 

Mit diesem technische Fortschritt einhergehend wurden Verfahren entwickelt, welche alle möglichen Kombinationen von Prädiktoren berücksichtigen und gegeneinander testen. In einem ersten Teil dieser Arbeit werden existierende Verfahren dargestellt. 

Es können beliebig viele potentiel erklärende Variablen erhoben werden um sich komplexe Model generieren zu lassen. Menschen tendieren zu glauben, dass komplexe Probleme komplexe Lösungen benötigen. Die Forschung zeigt jedoch, dass gerade das Umgekehrte der Fall ist \cite[p.3]{armstrong2011illusions}. 
Insbesondere Gigerenzer demonstrierte eindrucksvoll wie mit einfachen Rekognitionsheuristiken bessere Vorhersagen gemacht werden konnten als mit komplexen statistischen Modellen \cite{borges1999can}. Komplexe Modelle können sehr gute Vorhersagen liefern innerhalb des Testdatensatzes, doch oft scheitern die Vorhersage bei der Generalisierung. Der zweite Teil befasst sich mit dem Problem der Überanpassung komplexer Modelle und diskutiert mögliche Lösungsansätze.

\section{Multikollinearität -  Wenn Prädiktoren zusammenhängen}
Die multiple Regression setzt die Unabhängigkeit von Prädikatoren voraus. Das will heissen, dass Prädikatoren, welche in die Regressionsgleichung aufgenommen werden, nicht das selbe messen. ``Unter Multikollinearität versteht man die wechselseitige, lineare Abhängigkeit von Variablen im Kontext multivariater Verfahren.'' \cite[p. 453]{bortz2011}

Im Kontext der psychologischen Forschung ist eine gewisse Korrelation unter den Prädikatoren nahezu unausschliessbar. Als kleines Beispiel nehmen wir mal an, dass ein Entwicklungspsychologe die durchschnittliche Schulnote aufgrund eines Intelligenztests, eines Tests der Daueraufmerksamket, dem sozioökonomischen Status der Eltern sowie des Alters vorhersagen will. Es stellt sich nun die Frage ob bei der Konstruktion der beiden Tests eine korrelation berücksichtigt wurde. Wenn nicht besteht die Gefahr, dass der Intelligenztest zu einem grossen Teil auch schlicht die Daueraufmerksamkeit misst. Ist dies der Fall, wäre das ein Paradebeispiel einer Kollinearität.

Die $\beta$-Gewichte $b_i$ einer multiplen Regression mit zwei Prädikatoren werden wie folgt ermittelt:

\begin{equation}
b_1 = \frac{r_{1c}^2 - r_{2c}\cdot r_{12}}{1-r_{12}^2}
\label{eq:beta1}
\end{equation}

\begin{equation}
b_2 = \frac{r_{2c}^2 - r_{1c}\cdot r_{12}}{1-r_{12}^2}
\label{eq:beta2}
\end{equation}

Gut ersichtlich ist nun, dass zusätzlich zur Korrelation mit der Kriteriumsvariable $r_{ic}$, in den Gleichungen \ref{eq:beta1} und \ref{eq:beta2} die Korrelation zwischen den beiden Prädikatoren sowohl im Nenner als auch im Zähler mit einfliessen. Dadurch wird die Interpretation der $\beta$-Gewichte erschwert, da diese mit zunehmender Korrelation der Prädikatoren verzerrt werden. Am deutlichsten wird dies, wenn der eine Prädikator eine linearkombination des anderen Prädikator darstellt. In diesem Fall haben wir eine perfekte Korrelation, also $r_{12} = 1$, was vorkommen kann, wenn beispielsweise ein Prädikator versehentlich zweimal in die Gleichung einfliesst.

\begin{equation}
\lim_{r_{12} \to 1} \frac{r_{2c}^2 - r_{1c}\cdot r_{12}}{1-r_{12}^2} =  \lim_{r_{12} \to 1} \frac{r_{1c}^2 - r_{2c}\cdot r_{12}}{1-r_{12}^2} = \infty
\label{eq:linearcomb}
\end{equation}
 
Eine solche Linearkombination führt dazu, dass die $\beta$-Gewichte gar nicht berechnet werden können, wie aus der Gleichung \ref{eq:linearcomb} ersichtlich wird.

Literatur: \cite[p. 452ff]{bortz2011}, \cite{jacob2003applied}, \cite[p. 56ff]{harrell2001regression}

Detecting multicollinearity
Techniques that examine the interdependence of the data by examining what
would be called the design matrix in controlled experiments are becoming
widely used. These include examination of the eigenvalues (the variance of
each independent principal component), the eigenvectors (the matrix of
coefficients used to obtain the independent principal components), the tol-
erance (1 minus the squared multiple correlation between that X1 and the
remaining X variables), and the variance inflation factors (VIF) (the recip-
rocals of the tolerances) (Belsey et al., 1980; Gunst and Mason, 1980; Afifi
and Clark, 1984; Neter et al., 1985 ). Dillon and Goldstein (1984) advocated
a similar analysis called "singular-value decomposition". \cite[p. 3]{lafi1992explanation}
\subsection{Korrelation}
\blindtext

\subsection{Bestimmtheitsmass} 
\blindtext

\subsection{Konditionsindex}
\blindtext

\section{Interaktionen}
Literatur: \citeA{cortina1994interaction}

\blindtext

\section{Nichtlinearitäten}
Literatur: \citeA{cortina1994interaction}

\blindtext


\section{Modelloptimierung - Was zusammenhängt gehört rein, was einigermassen unbedeutend ist raus.}
\subsection{Hypothesentestung}
Literatur: \citeA{pasha2002selection}, \citeA{mundry2009stepwise}

\blindtext
\subsection{AIC}
Literatur : \citeA{weakliem2004introduction}

\blindtext
\subsection{BIC}
Literatur : \citeA{weakliem2004introduction}

\blindtext
\subsection{Kreuzvalidierung}
Literatur : \citeA{arlot2010survey}

\blindtext

\section{Data-Mining - aus einem Datenberg etwas Wertvolles extrahieren}
Literatur: \citeA{hand2001principles}, \citeA{fischerstatistik}

\blindtext
\section{Zusammenfassung / Diskussion}
\blindtext
\blindtext

\newpage
\bibliographystyle{apacite} 
\bibliography{literature}
\newpage
\section{Anhang}
\blindtext
\newpage
\section{Selbstständigkeitserklärung}
\blindtext
\begin{comment}
I declare that this work titled ``\title'' has been composed by myself, and describes my own work, unless otherwise acknowledged in the 
text.  

If the paper has been authored by more than one person, I confirm that all parts of the paper have been clearly assigned to the respective author.

This work has not been and will not be submitted  for any other degree or the obtaining of ECTS points at the University of Zurich or any other institution of higher education. 

All sentences or passages quoted in this paper  from other people's work have been specifically acknowledged by clear cross-referencing to author, work and page(s). Any illustrations which are not the work of the author have been used with the  explicit permission of the originator and are specifically acknowledged.  

I understand that failure to specifically acknowledge all used work amounts to plagiarism and will be considered grounds for failure and will have judicial and disciplinary consequences according §7ff of the ``Disziplinarordnung der Universität Zürich'' as well as § 36 of the ``Rahmenordnung für das Studium in den Bachelor- und Master-Studiengängen der Philosophischen Fakultät der Universität Zürich''. 

With my signature I declare the accuracy of these specifications.
\\
\\
Name: Markus Graf\\
Matriculation number:  08-91271-9\\
\\
\\
\\
\\
\\
\\
............................................................................\\
Zürich, \today
\end{comment}


\end{document}
